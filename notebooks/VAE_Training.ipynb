{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../fashion_ml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fashion_vae import FashionVAE\n",
    "from data_loader import EZ_Dataloader\n",
    "from experiment import FashionML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = EZ_Dataloader(\"FashionMNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.build_train_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.build_val_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = dl.train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc0286806a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "a = t[12342][0]\n",
    "plt.imshow(torch.squeeze(a), cmap = \"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvae = FashionVAE(h_dims = [32,64,128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionVAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(3, 3), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(3, 3), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(3, 3), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (fc_log_var): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (decoder_latent_space): Linear(in_features=2, out_features=128, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(3, 3), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(3, 3), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (final_layer): Sequential(\n",
       "    (0): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(3, 3), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Conv2d(32, 1, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fml = FashionML(fvae, dl, adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fml.epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff401fdbe664fc4a3a0fb7dbb1b62df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.420644521713257\n",
      "Validation Loss: 2.982944965362549\n",
      "Validation Loss: 2.860074520111084\n",
      "Validation Loss: 2.8028180599212646\n",
      "Validation Loss: 2.7614126205444336\n",
      "Validation Loss: 2.7502102851867676\n",
      "Validation Loss: 2.7453620433807373\n",
      "Validation Loss: 2.7009029388427734\n",
      "Validation Loss: 2.6860644817352295\n",
      "Validation Loss: 2.6624391078948975\n",
      "Validation Loss: 2.6554625034332275\n",
      "Validation Loss: 2.6535701751708984\n",
      "Validation Loss: 2.6347761154174805\n",
      "Validation Loss: 2.6349878311157227\n",
      "Validation Loss: 2.6446847915649414\n",
      "Validation Loss: 2.639855146408081\n",
      "Validation Loss: 2.608280658721924\n",
      "Validation Loss: 2.5985331535339355\n",
      "Validation Loss: 2.610464572906494\n",
      "Validation Loss: 2.6016502380371094\n",
      "Validation Loss: 2.6179306507110596\n"
     ]
    }
   ],
   "source": [
    "fml.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent, t = fvae.sample(1, include_locations= True)\n",
    "\n",
    "print(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.squeeze(t.detach().cpu()), cmap = \"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(fml.model.state_dict(), \"../models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = []\n",
    "for i in range(20):\n",
    "    i1 = (i-10)/2\n",
    "    row = []\n",
    "    for j in range(20):\n",
    "        i2 = (j-10)/2\n",
    "        \n",
    "        row.append(torch.squeeze(fml.model.sample_latent((i1,i2))))\n",
    "    \n",
    "    img.append(torch.cat(row, axis = 1))\n",
    "\n",
    "img = torch.cat(img, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,12))\n",
    "plt.imshow(img)\n",
    "plt.axes(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3],\n",
    "                 [1,2,3]])\n",
    "b = torch.tensor([[1,2,3],\n",
    "                 [1,2,3]])\n",
    "\n",
    "c = torch.cat([a,b], axis = 1)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sailon-ml",
   "language": "python",
   "name": "sailon-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
